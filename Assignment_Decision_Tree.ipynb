{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "  - A Decision Tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It models decisions and their potential consequences in a tree-like structure, similar to a flowchart.\n",
        "In the context of classification, a Decision Tree works by recursively partitioning the input data into subsets based on the values of different features, aiming to create increasingly homogeneous groups with respect to the target class"
      ],
      "metadata": {
        "id": "d49cAaGc_Vpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "  - Gini Impurity and Entropy are two mathematical measures used in decision tree algorithms to determine the best way to split a node into purer, more homogeneous subsets. A pure node contains data points from only one class, while an impure node contains a mix of data from different classes. The goal of a decision tree is to find splits that minimize impurity.\n",
        "      - In a decision tree algorithm, Gini Impurity and Entropy are used as splitting criteria to find the optimal feature and threshold for dividing the data at each node.\n",
        "\n",
        "          - Calculate parent node impurity: The algorithm first computes the impurity (Gini or Entropy) of the current node.\n",
        "          - Evaluate potential splits: For every feature and its possible values, the algorithm considers splitting the data and creating child nodes.\n",
        "          - Calculate weighted child impurity: For each potential split, it calculates the weighted average impurity of the resulting child nodes. The weighted average is determined by the number of instances in each child node relative to the parent.\n",
        "          - Determine the best split: The split that results in the lowest weighted impurity (Gini) or the highest Information Gain (based on Entropy) is chosen as the optimal split for that node."
      ],
      "metadata": {
        "id": "4ER5P_OQ_rG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "  - Pre-pruning stops a decision tree from growing by setting stopping criteria during training, while post-pruning builds a full tree and then trims it. A practical advantage of pre-pruning is its efficiency because it avoids the computational cost of growing an oversized tree, whereas a practical advantage of post-pruning is that it can lead to more accurate results by fine-tuning the final model after full growth, as seen in methods like cost-complexity pruning."
      ],
      "metadata": {
        "id": "KQWa5ONCAuYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "  - Information Gain measures the reduction in entropy (uncertainty) in a dataset after splitting it by a feature, and it's important for choosing the best split in decision trees because it identifies the feature that best separates the data into pure, homogeneous classes. Attributes with high Information Gain reduce uncertainty the most, leading to more accurate and informative decision trees, which helps in constructing a more efficient and effective model.  "
      ],
      "metadata": {
        "id": "U256tP4XBW-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "  - Common applications of decision trees include healthcare for disease diagnosis, finance for risk assessment, and marketing for customer segmentation. Their main advantages are that they are easy to understand and require minimal data preparation. However, a major limitation is their tendency to overfit the data, meaning they can become too complex and perform poorly on new data.\n",
        "      - Simple to understand: Decision trees are easy to interpret and visualize, making them simple to explain to non-technical audiences.\n",
        "      - Minimal data preparation: They require less data preprocessing, such as cleaning or normalization, compared to many other algorithms.\n",
        "      - Handles mixed data types: Decision trees can handle both numerical and categorical data.\n",
        "      - Transparent logic: They provide a clear, step-by-step representation of the decision-making process.\n",
        "      - Versatile: They can be used for both classification (predicting a category) and regression (predicting a continuous value)."
      ],
      "metadata": {
        "id": "5ZEK9bZqBm6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "      - Load the Iris Dataset\n",
        "      - Train a Decision Tree Classifier using the Gini criterion\n",
        "      - Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "Aa68o6k6HyGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWr9GWYn-1Pj",
        "outputId": "82b84b35-ae70-4a66-edb4-1378e64f6b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Iris Dataset...\n",
            "Training samples: 120, Testing samples: 30\n",
            "\n",
            "Training Decision Tree Classifier (Criterion: Gini)...\n",
            "Training complete.\n",
            "\n",
            "----------------------------------------\n",
            "Model Accuracy on Test Set: 0.9333\n",
            "----------------------------------------\n",
            "\n",
            "Feature Importances (Contribution to Model Prediction):\n",
            "          Feature  Importance\n",
            "petal length (cm)    0.558568\n",
            " petal width (cm)    0.406015\n",
            " sepal width (cm)    0.029167\n",
            "sepal length (cm)    0.006250\n",
            "\n",
            "Interpretation: Higher importance means the feature was more crucial in splitting the nodes of the decision tree.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_and_evaluate_iris_decision_tree():\n",
        "\n",
        "    print(\"Loading Iris Dataset...\")\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    feature_names = iris.feature_names\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\\n\")\n",
        "\n",
        "    print(\"Training Decision Tree Classifier (Criterion: Gini)...\")\n",
        "    dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "    dt_classifier.fit(X_train, y_train)\n",
        "    print(\"Training complete.\\n\")\n",
        "\n",
        "    y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    importances = dt_classifier.feature_importances_\n",
        "    feature_importances_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\nFeature Importances (Contribution to Model Prediction):\")\n",
        "    print(feature_importances_df.to_string(index=False))\n",
        "    print(\"\\nInterpretation: Higher importance means the feature was more crucial in splitting the nodes of the decision tree.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_iris_decision_tree()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "  -  Load the Iris Dataset\n",
        "  -  Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "7r6xy-ZKKDZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compare_decision_tree_depths():\n",
        "\n",
        "    print(\"Loading Iris Dataset...\")\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\\n\")\n",
        "\n",
        "    print(\"Training Fully-Grown Decision Tree (max_depth=None)...\")\n",
        "    dt_full = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
        "    dt_full.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_full = dt_full.predict(X_test)\n",
        "    accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "    print(\"Training Decision Tree with max_depth=3...\")\n",
        "    dt_depth3 = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "    dt_depth3.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_depth3 = dt_depth3.predict(X_test)\n",
        "    accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 55)\n",
        "    print(\"Decision Tree Classifier Accuracy Comparison\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"1. Fully-Grown Tree (max_depth=None): {accuracy_full:.4f}\")\n",
        "    print(f\"2. Max Depth 3 Tree (max_depth=3):    {accuracy_depth3:.4f}\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    if accuracy_full > accuracy_depth3:\n",
        "        print(\"\\nNote: The fully-grown tree achieved higher accuracy on this test set.\")\n",
        "    elif accuracy_full < accuracy_depth3:\n",
        "        print(\"\\nNote: The max_depth=3 tree performed slightly better on this test set.\")\n",
        "    else:\n",
        "        print(\"\\nNote: Both models achieved the same accuracy on this test set.\")\n",
        "    print(\"Using a constrained depth (like max_depth=3) is a form of pre-pruning to mitigate overfitting.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    compare_decision_tree_depths()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jY-UhBjJ0aE",
        "outputId": "23155eeb-c0ec-43ce-e8ef-726a736470b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Iris Dataset...\n",
            "Training samples: 120, Testing samples: 30\n",
            "\n",
            "Training Fully-Grown Decision Tree (max_depth=None)...\n",
            "Training Decision Tree with max_depth=3...\n",
            "\n",
            "=======================================================\n",
            "Decision Tree Classifier Accuracy Comparison\n",
            "=======================================================\n",
            "1. Fully-Grown Tree (max_depth=None): 0.9333\n",
            "2. Max Depth 3 Tree (max_depth=3):    0.9667\n",
            "=======================================================\n",
            "\n",
            "Note: The max_depth=3 tree performed slightly better on this test set.\n",
            "Using a constrained depth (like max_depth=3) is a form of pre-pruning to mitigate overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "  -  Load the Boston Housing Dataset\n",
        "  -  Train a Decision Tree Regressor\n",
        "  -  Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "1VlW9UPSKv97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def train_and_evaluate_boston_decision_tree():\n",
        "\n",
        "    print(\"Loading Boston Housing Dataset...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        boston = fetch_openml(name='boston', version=1, as_frame=True, parser='auto')\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Boston dataset via fetch_openml: {e}\")\n",
        "        print(\"Please ensure you have an internet connection and compatible scikit-learn version.\")\n",
        "        return\n",
        "\n",
        "    X = boston.data\n",
        "    y = boston.target\n",
        "\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\\n\")\n",
        "\n",
        "    print(\"Training Decision Tree Regressor...\")\n",
        "\n",
        "    dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "    dt_regressor.fit(X_train, y_train)\n",
        "    print(\"Training complete.\\n\")\n",
        "\n",
        "    y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(\"=\" * 45)\n",
        "    print(f\"Mean Squared Error (MSE) on Test Set: {mse:.4f}\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    importances = dt_regressor.feature_importances_\n",
        "\n",
        "    feature_importances_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\nFeature Importances (Contribution to Price Prediction):\")\n",
        "    print(feature_importances_df.to_string(index=False))\n",
        "    print(\"\\nInterpretation: Features with higher importance are more influential in determining the predicted house price.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate_boston_decision_tree()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jYD_Q17KueP",
        "outputId": "c1b4ef05-40a3-4554-aba3-58d1467fd548"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Boston Housing Dataset...\n",
            "Training samples: 404, Testing samples: 102\n",
            "\n",
            "Training Decision Tree Regressor...\n",
            "Training complete.\n",
            "\n",
            "=============================================\n",
            "Mean Squared Error (MSE) on Test Set: 10.4161\n",
            "=============================================\n",
            "\n",
            "Feature Importances (Contribution to Price Prediction):\n",
            "Feature  Importance\n",
            "     RM    0.600326\n",
            "  LSTAT    0.193328\n",
            "    DIS    0.070688\n",
            "   CRIM    0.051296\n",
            "    NOX    0.027148\n",
            "    AGE    0.013617\n",
            "    TAX    0.012464\n",
            "PTRATIO    0.011012\n",
            "      B    0.009009\n",
            "  INDUS    0.005816\n",
            "     ZN    0.003353\n",
            "    RAD    0.001941\n",
            "   CHAS    0.000002\n",
            "\n",
            "Interpretation: Features with higher importance are more influential in determining the predicted house price.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "  -  Load the Iris Dataset\n",
        "  -  Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "  -  Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "YCzTNI9bLjgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def tune_decision_tree_with_gridsearch():\n",
        "\n",
        "    print(\"Loading Iris Dataset...\")\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
        "    print(\"Preparing Grid Search for Hyperparameter Tuning...\\n\")\n",
        "\n",
        "    param_grid = {\n",
        "\n",
        "        'max_depth': list(range(2, 11)),\n",
        "\n",
        "        'min_samples_split': [2, 3, 5, 7, 10]\n",
        "    }\n",
        "\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=dt,\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"Starting Grid Search (testing parameter combinations)...\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"Grid Search complete.\\n\")\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    best_cv_score = grid_search.best_score_\n",
        "\n",
        "    best_dt_model = grid_search.best_estimator_\n",
        "    y_pred_test = best_dt_model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "\n",
        "    print(\"=\" * 55)\n",
        "    print(\"GridSearchCV Results for Decision Tree Tuning\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"Best Hyperparameters Found: {best_params}\")\n",
        "    print(f\"Best Cross-Validation Accuracy: {best_cv_score:.4f}\")\n",
        "    print(\"-\" * 55)\n",
        "    print(f\"Accuracy on Independent Test Set: {test_accuracy:.4f}\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tune_decision_tree_with_gridsearch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0XGX_QfLhma",
        "outputId": "a69e1c6c-d725-4c01-8ac0-26cdeb407b9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Iris Dataset...\n",
            "Training samples: 120, Testing samples: 30\n",
            "Preparing Grid Search for Hyperparameter Tuning...\n",
            "\n",
            "Starting Grid Search (testing parameter combinations)...\n",
            "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
            "Grid Search complete.\n",
            "\n",
            "=======================================================\n",
            "GridSearchCV Results for Decision Tree Tuning\n",
            "=======================================================\n",
            "Best Hyperparameters Found: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Best Cross-Validation Accuracy: 0.9417\n",
            "-------------------------------------------------------\n",
            "Accuracy on Independent Test Set: 0.9333\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "  -  Handle the missing values\n",
        "  -  Encode the categorical features\n",
        "  -  Train a Decision Tree model\n",
        "  -  Tune its hyperparameters\n",
        "  -  Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "        - As a data scientist predicting disease for a healthcare company, the following steps would be used to build and evaluate a Decision Tree model. Given the mixed data types and missing values, preprocessing techniques must be carefully selected to avoid biasing the results.\n",
        "        - Handle the missing values\n",
        "            - Begin by performing an exploratory data analysis (EDA) to understand the extent and patterns of missing data. Visualize the missing data using a library like missingno to identify if the values are missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR).\n",
        "        - Encode the categorical features\n",
        "            - Decision Trees in scikit-learn require numerical input, so categorical features must be converted before training.\n",
        "        - Train a Decision Tree model\n",
        "            - Data splitting: First, split the dataset into a training set and a testing set. A common split is 80% for training and 20% for testing. Stratified splitting should be used to ensure the proportion of the target disease class is maintained in both sets, especially in cases of class imbalance.\n",
        "            - Training: Initialize and train the Decision Tree classifier on the training data. The model learns a series of if-then-else decision rules from the features to predict the target variable.\n",
        "            - Handling large datasets: If the dataset is too large to fit into memory, consider using libraries like Dask or PySpark to process the data in chunks or parallelize computations. Alternatively, training can be done on a representative, stratified sample of the data.\n",
        "        - Tune its hyperparameters\n",
        "            - Hyperparameters are set before training and control the learning process. Tuning is critical to prevent overfitting and improve performance on unseen data.\n",
        "        - Evaluate its performance\n",
        "            - After tuning, the model's performance must be rigorously evaluated on the held-out test set to ensure its reliability. In a healthcare setting, a single metric is not sufficient due to the high costs associated with false predictions."
      ],
      "metadata": {
        "id": "k0Fxki7TMa0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R77daBtOMF7L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}