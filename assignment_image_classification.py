# -*- coding: utf-8 -*-
"""Assignment : Image Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eQ8BqhcOhjk14vsthkU4PLCUmskPgaTP

1. What is a Convolutional Neural Network (CNN), and how does it differ from traditional fully connected neural networks in terms of architecture and performance on image data?
  - A Convolutional Neural Network (CNN) is a specialized type of deep learning architecture designed primarily to process data with a grid-like topology, such as images. While traditional neural networks treat images as a flat list of numbers, CNNs are built to understand the spatial hierarchy of visual data—recognizing pixels as parts of edges, edges as parts of shapes, and shapes as parts of objects.
  - Fully Connected (Dense) Networks
In a traditional Multi-Layer Perceptron (MLP), every input neuron is connected to every neuron in the next layer.
  - CNNs use three key architectural principles to overcome these hurdles: Local Receptive Fields, Shared Weights, and Spatial Pooling.

2. Discuss the architecture of LeNet-5 and explain how it laid the foundation for modern deep learning models in computer vision. Include references to its original research paper.
  - LeNet-5, introduced by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner in their 1998 seminal paper, "Gradient-Based Learning Applied to Document Recognition," is considered the "ancestor" of all modern Convolutional Neural Networks (CNNs).
While we take these concepts for granted today, LeNet-5 was the first to successfully demonstrate that a machine could learn to extract features directly from raw pixels without manual engineering.
  - The Architecture of LeNet-5
LeNet-5 follows a specific sequence of layers designed to recognize handwritten digits (the MNIST dataset). It consists of seven layers (excluding the input), primarily alternating between Convolutional and Subsampling (Pooling) layers.
  - Foundations for Modern Deep Learning
LeNet-5 didn't just solve a niche problem (reading checks for banks); it introduced the "Golden Rules" that govern modern architectures like ResNet or VGG.

3. Compare and contrast AlexNet and VGGNet in terms of design principles, number of parameters, and performance. Highlight key innovations and limitations of each.
  - AlexNet and VGGNet are both landmark architectures in the history of computer vision, representing the "Deep Learning Revolution." While AlexNet proved that deep CNNs could work, VGGNet refined that idea into a systematic design philosophy.
  - AlexNet (The Pioneer)
Introduced by Alex Krizhevsky in 2012, AlexNet was designed to be deep enough to learn complex features but was limited by the hardware of its time.
  - VGGNet (The Perfectionist)
Introduced by the Visual Geometry Group at Oxford in 2014, VGGNet’s design principle was "simplicity through depth."

4.  What is transfer learning in the context of image classification? Explain how it helps in reducing computational costs and improving model performance with limited data.
  - Transfer Learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second, related task.

In image classification, this typically means taking a powerhouse model (like VGG-16, ResNet, or Inception) that has already been trained on a massive dataset (like ImageNet, which contains over 14 million images across 1,000 categories) and "re-purposing" its knowledge for your specific, often smaller, dataset.

  - Reducing Computational Costs
Training a deep neural network from scratch is incredibly "expensive" in terms of time and hardware.

      - Weight Initialization: Instead of starting with random numbers (noise) and spending weeks of GPU time finding the right values, you start with weights that already "know" how to interpret images.

      - Fewer Trainable Parameters: Because you freeze the convolutional base, the backpropagation algorithm only needs to update the weights in your new, smaller classification head.

      - Faster Convergence: Because the model starts with a "head start," it reaches high accuracy in a fraction of the epochs (iterations) required for a fresh model.
  - Improving Performance with Limited Data
      This is the "killer app" of transfer learning. If you only have 100 images of a rare medical condition, a deep CNN trained from scratch will almost certainly overfit—it will memorize your 100 images rather than learning general features.

      - Knowledge Transfer: The model brings in "prior knowledge" from ImageNet. It already knows what a "circular shape" or "grainy texture" looks like.

      - Feature Robustness: The features learned on millions of images are much more robust and generalized than anything a model could learn from a tiny dataset.

      - Fine-Tuning: Once the new classifier is stable, you can "unfreeze" the top layers of the convolutional base and train them at a very low learning rate. This allows the model to slightly adjust its high-level feature detection to better suit your specific images.

5. Describe the role of residual connections in ResNet architecture. How do they address the vanishing gradient problem in deep CNNs?
  - The introduction of ResNet (Residual Network) by Kaiming He et al. in 2015 was a watershed moment for deep learning. Before ResNet, adding more layers to a network often made it perform worse—not because of overfitting, but because the network became physically unable to train. Residual connections solved this by changing how information flows through the architecture.
  - Addressing the Vanishing Gradient Problem As networks get deeper, the gradients (the signals used to update weights) must be multiplied repeatedly through many layers during backpropagation.The Problem in Plain NetworksIn a traditional deep network, if the weights or activations are small, the gradient shrinks exponentially as it travels back toward the input layers. By the time it reaches the first few layers, the gradient is effectively zero ($0.0000...$), meaning those layers never "learn" or update. This is the Vanishing Gradient Problem.

6. Implement the LeNet-5 architectures using Tensorflow or PyTorch to classify the MNIST dataset. Report the accuracy and training time.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import time

transform = transforms.Compose([
    transforms.Pad(2),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),
            nn.Tanh(),
            nn.AvgPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),
            nn.Tanh(),
            nn.AvgPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),
            nn.Tanh()
        )
        self.classifier = nn.Sequential(
            nn.Linear(in_features=120, out_features=84),
            nn.Tanh(),
            nn.Linear(in_features=84, out_features=10),
        )

    def forward(self, x):
        x = self.feature_extractor(x)
        x = torch.flatten(x, 1)
        logits = self.classifier(x)
        return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LeNet5().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print(f"Training on {device}...")
start_time = time.time()

model.train()
for epoch in range(5):
    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

training_time = time.time() - start_time

model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"\nFinal Results:")
print(f"Accuracy: {accuracy:.2f}%")
print(f"Training Time: {training_time:.2f} seconds")

"""7. Use a pre-trained VGG16 model (via transfer learning) on a small custom dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model. Include your code and result discussion."""

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
import tensorflow_datasets as tfds

def preprocess(image, label):
    image = tf.image.resize(image, (224, 224))
    image = tf.keras.applications.vgg16.preprocess_input(image)
    return image, label

dataset, info = tfds.load('tf_flowers', with_info=True, as_supervised=True, split='train[:80%]')
val_dataset = tfds.load('tf_flowers', as_supervised=True, split='train[80%:]')

train_ds = dataset.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)
val_ds = val_dataset.map(preprocess).batch(32)

base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(5, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
print("Starting initial training of the top layers...")
model.fit(train_ds, validation_data=val_ds, epochs=1)

base_model.trainable = True
for layer in base_model.layers[:-4]:
    layer.trainable = False

model.compile(optimizer=optimizers.Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
print("\nStarting fine-tuning...")
history = model.fit(train_ds, validation_data=val_ds, epochs=1)

"""8.  Write a program to visualize the filters and feature maps of the first convolutional layer of AlexNet on an example input image."""

import torch
import matplotlib.pyplot as plt
from torchvision import models, transforms
from PIL import Image
import requests
from io import BytesIO

model = models.alexnet(weights='AlexNet_Weights.IMAGENET1K_V1').eval()

first_layer = model.features[0]
filters = first_layer.weight.data.cpu()

def plot_filters(filters):
    f_min, f_max = filters.min(), filters.max()
    filters = (filters - f_min) / (f_max - f_min)

    fig = plt.figure(figsize=(12, 8))
    fig.suptitle("AlexNet: First Layer Filters (11x11)", fontsize=16)
    for i in range(64):
        plt.subplot(8, 8, i+1)

        plt.imshow(filters[i].numpy().transpose(1, 2, 0))
        plt.axis('off')
    plt.show()

plot_filters(filters)

url = "https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg"
response = requests.get(url)
img = Image.open(BytesIO(response.content)).convert('RGB')

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(img).unsqueeze(0)

with torch.no_grad():
    feature_maps = first_layer(input_tensor)

def plot_feature_maps(feature_maps):
    fig = plt.figure(figsize=(12, 8))
    fig.suptitle("AlexNet: First Layer Feature Maps (Activations)", fontsize=16)
    for i in range(64):
        plt.subplot(8, 8, i+1)
        plt.imshow(feature_maps[0, i].cpu().numpy(), cmap='gray')
        plt.axis('off')
    plt.show()

plot_feature_maps(feature_maps)

"""9. Train a GoogLeNet (Inception v1) or its variant using a standard dataset like CIFAR-10. Plot the training and validation accuracy over epochs and analyze overfitting or underfitting."""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import googlenet
import matplotlib.pyplot as plt
import time

transform_train = transforms.Compose([
    transforms.Resize(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = googlenet(num_classes=10, aux_logits=False).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

history = {'train_acc': [], 'val_acc': []}

for epoch in range(15):
    model.train()
    correct, total = 0, 0
    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    train_acc = 100 * correct / total
    history['train_acc'].append(train_acc)

    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    val_acc = 100 * correct / total
    history['val_acc'].append(val_acc)
    scheduler.step()
    print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")

plt.figure(figsize=(10, 5))
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('GoogLeNet on CIFAR-10: Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)
plt.show()

"""10.  You are working in a healthcare AI startup. Your team is tasked with developing a system that automatically classifies medical X-ray images into normal, pneumonia, and COVID-19. Due to limited labeled data, what approach would you suggest using among CNN architectures discussed (e.g., transfer learning with ResNet or Inception variants)? Justify your approach and outline a deployment strategy for production use."""

import torch
import torch.nn as nn
from torchvision import models, transforms

class XRayClassifier(nn.Module):
    def __init__(self, num_classes=3):
        super(XRayClassifier, self).__init__()

        self.backbone = models.densenet121(weights='DenseNet121_Weights.IMAGENET1K_V1')

        for param in self.backbone.parameters():
            param.requires_grad = False

        num_ftrs = self.backbone.classifier.in_features
        self.backbone.classifier = nn.Sequential(
            nn.Linear(num_ftrs, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.backbone(x)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomRotation(10),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

model = XRayClassifier(num_classes=3)
print(f"Model initialized. Classifier input features: {model.backbone.classifier[0].in_features}")

