{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "  - Ensemble learning combines multiple machine learning models to produce a single, more robust and accurate predictive model. The key idea is that the \"wisdom of the crowd\" can achieve better results than any single model by leveraging the diversity of different \"weak learners\" to offset individual errors and limitations."
      ],
      "metadata": {
        "id": "VB3JDcpGQzfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?\n",
        "  - Bagging builds models in parallel on different data subsets to reduce variance, while boosting builds models sequentially, with each new model correcting the errors of the previous one to reduce bias. The key differences are that bagging uses random data samples with replacement and parallel training, giving equal importance to each model, whereas boosting focuses on errors and trains models sequentially, giving more weight to poorly performing ones.  "
      ],
      "metadata": {
        "id": "-bpYD2BkS3u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "  - Bootstrap sampling is the process of repeatedly drawing random samples from a dataset with replacement to create multiple unique training subsets. In Bagging (Bootstrap Aggregating) methods like Random Forest, bootstrap sampling is used to create these diverse datasets, which are then used to train multiple base models (like decision trees) independently. By averaging the predictions from these models, Bagging reduces variance and improves overall model stability and accuracy."
      ],
      "metadata": {
        "id": "ySfmiFujTI8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "  - Out-of-bag (OOB) samples are data points from the training set that are not included in the bootstrap sample used to train a specific base model, such as a decision tree in a random forest. The OOB score is a performance metric calculated by using these OOB samples to make predictions for each model and then evaluating the model's accuracy on them. This provides an internal validation measure, similar to a cross-validation score, without needing a separate validation set."
      ],
      "metadata": {
        "id": "wtX8G8k5TTc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "  - Feature importance analysis in a single Decision Tree and a Random Forest both aim to identify the most influential features in a dataset, but they differ in their methodology and robustness.\n",
        "      - Single Decision Tree:\n",
        "          - Feature importance in a single decision tree is typically based on the reduction in impurity (e.g., Gini impurity or entropy) achieved by splitting on that feature. Features that lead to larger reductions in impurity are considered more important.\n",
        "\n",
        "      - Random Forest:\n",
        "          - Random Forests calculate feature importance by averaging the impurity reduction (or other metrics like permutation importance) across all the individual decision trees within the forest. For instance, the Mean Decrease in Impurity (MDI) sums the impurity reductions attributed to a feature across all trees and then averages them. Permutation importance involves shuffling a feature's values and measuring the decrease in model performance.\n"
      ],
      "metadata": {
        "id": "RunocHLeTc1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "  -  Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "  -  Train a Random Forest Classifier\n",
        "  -  Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "51lq6YQxT7xt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnMNM-EHQpwB",
        "outputId": "dac6ca09-1f1d-461a-fdd3-04a2a022f35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Breast Cancer dataset...\n",
            "Training RandomForestClassifier...\n",
            "\n",
            "Top 5 most important features:\n",
            "24. worst area                     (Importance: 0.1394)\n",
            "28. worst concave points           (Importance: 0.1322)\n",
            "8. mean concave points            (Importance: 0.1070)\n",
            "21. worst radius                   (Importance: 0.0828)\n",
            "23. worst perimeter                (Importance: 0.0808)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"Loading Breast Cancer dataset...\")\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "print(\"Training RandomForestClassifier...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_df, y)\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values(\n",
        "    by='Importance', ascending=False\n",
        ")\n",
        "\n",
        "top_n = 5\n",
        "print(f\"\\nTop {top_n} most important features:\")\n",
        "\n",
        "for index, row in feature_importance_df.head(top_n).iterrows():\n",
        "    print(f\"{index + 1}. {row['Feature']:<30} (Importance: {row['Importance']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "  -  Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "  -  Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "AeOVN_1ZUs8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Loading Iris dataset...\")\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Dataset split: {len(X_train)} training samples, {len(X_test)} testing samples.\")\n",
        "\n",
        "print(\"\\n--- Training Single Decision Tree ---\")\n",
        "\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "\n",
        "y_pred_single = single_tree.predict(X_test)\n",
        "accuracy_single = accuracy_score(y_test, y_pred_single)\n",
        "print(f\"Single Decision Tree Accuracy: {accuracy_single:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Bagging Classifier ---\")\n",
        "\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n",
        "\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "print(f\"Single Tree Accuracy:   {accuracy_single:.4f}\")\n",
        "print(f\"Bagging Model Accuracy: {accuracy_bagging:.4f}\")\n",
        "\n",
        "if accuracy_bagging > accuracy_single:\n",
        "    print(\"\\nResult: The Bagging Classifier provided a higher or equal accuracy, demonstrating the benefit of ensemble learning.\")\n",
        "elif accuracy_bagging == accuracy_single:\n",
        "    print(\"\\nResult: Both models achieved the same accuracy on this dataset split.\")\n",
        "else:\n",
        "    print(\"\\nResult: The Single Decision Tree performed better in this specific test split.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHXmA0Y9UqQf",
        "outputId": "a572e079-bdca-407d-f57d-3d1d9e09493b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Iris dataset...\n",
            "Dataset split: 105 training samples, 45 testing samples.\n",
            "\n",
            "--- Training Single Decision Tree ---\n",
            "Single Decision Tree Accuracy: 0.9333\n",
            "\n",
            "--- Training Bagging Classifier ---\n",
            "Bagging Classifier Accuracy: 0.9333\n",
            "\n",
            "--- Model Comparison ---\n",
            "Single Tree Accuracy:   0.9333\n",
            "Bagging Model Accuracy: 0.9333\n",
            "\n",
            "Result: Both models achieved the same accuracy on this dataset split.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "  - Train a Random Forest Classifier\n",
        "  - Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "  - Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "akiMIn1oVI7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Loading Iris dataset...\")\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Dataset split: {len(X_train)} training samples, {len(X_test)} testing samples.\")\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [2, 5, 10, None],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "total_models = np.prod([len(v) for v in param_grid.values()]) * 5 # CV=5 by default\n",
        "print(f\"\\nSearching for best parameters using GridSearchCV (evaluating {total_models} fits)...\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"GridSearchCV Results\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "print(f\"Best Parameters Found: {grid_search.best_params_}\")\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "y_pred_best = best_rf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"Accuracy of the Tuned Model on Test Set: {final_accuracy:.4f}\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG7Dmw0yVGPh",
        "outputId": "464ee402-dd56-4431-d70d-f86c516d9893"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Iris dataset...\n",
            "Dataset split: 105 training samples, 45 testing samples.\n",
            "\n",
            "Searching for best parameters using GridSearchCV (evaluating 120 fits)...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "\n",
            "========================================\n",
            "GridSearchCV Results\n",
            "========================================\n",
            "Best Parameters Found: {'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Accuracy of the Tuned Model on Test Set: 0.9333\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "  -  Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "  -  Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "xJkt56PQYlR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Loading California Housing dataset...\")\n",
        "try:\n",
        "\n",
        "    housing = fetch_california_housing(as_frame=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure you have an internet connection if loading for the first time.\")\n",
        "    exit()\n",
        "\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"Dataset split: {len(X_train)} training samples, {len(X_test)} testing samples.\")\n",
        "\n",
        "print(\"\\n--- Training Bagging Regressor ---\")\n",
        "\n",
        "bagging_model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "print(f\"Bagging Regressor MSE: {mse_bagging:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Random Forest Regressor ---\")\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"Regression Model MSE Comparison\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Bagging Regressor MSE:    {mse_bagging:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "if mse_rf < mse_bagging:\n",
        "    print(\"\\nConclusion: The Random Forest Regressor achieved a lower Mean Squared Error (MSE) and is thus generally the better performing model on this data split.\")\n",
        "elif mse_bagging < mse_rf:\n",
        "    print(\"\\nConclusion: The Bagging Regressor achieved a lower Mean Squared Error (MSE).\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Both models performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RGVxaFEWHNc",
        "outputId": "396b7112-2507-430d-da35-ca71c94a291d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading California Housing dataset...\n",
            "Dataset split: 16512 training samples, 4128 testing samples.\n",
            "\n",
            "--- Training Bagging Regressor ---\n",
            "Bagging Regressor MSE: 0.2559\n",
            "\n",
            "--- Training Random Forest Regressor ---\n",
            "Random Forest Regressor MSE: 0.2554\n",
            "\n",
            "========================================\n",
            "Regression Model MSE Comparison\n",
            "========================================\n",
            "Bagging Regressor MSE:    0.2559\n",
            "Random Forest Regressor MSE: 0.2554\n",
            "========================================\n",
            "\n",
            "Conclusion: The Random Forest Regressor achieved a lower Mean Squared Error (MSE) and is thus generally the better performing model on this data split.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "  - Choose between Bagging or Boosting\n",
        "  - Handle overfitting\n",
        "  - Select base models\n",
        "  - Evaluate performance using cross-validation\n",
        "  - Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n"
      ],
      "metadata": {
        "id": "Q-ojLlBKZL-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"Generating simulated, imbalanced loan default data...\")\n",
        "X, y = make_classification(\n",
        "    n_samples=10000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.98, 0.02],\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scale_pos_weight = (len(y_train) - np.sum(y_train)) / np.sum(y_train)\n",
        "print(f\"Minority class proportion (Default): {np.mean(y) * 100:.2f}%\")\n",
        "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "\n",
        "    scale_pos_weight=scale_pos_weight\n",
        ")\n",
        "\n",
        "print(\"\\nStarting GridSearchCV with Stratified K-Fold for Hyperparameter Tuning...\")\n",
        "\n",
        "param_grid = {\n",
        "\n",
        "    'n_estimators': [100, 200],\n",
        "\n",
        "    'max_depth': [3, 5],\n",
        "\n",
        "    'learning_rate': [0.1, 0.01],\n",
        "}\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='roc_auc',\n",
        "    cv=skf,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Tuning and Evaluation Results (XGBoost)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"Best Parameters Found: {grid_search.best_params_}\")\n",
        "\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Test Set AUC-ROC Score: {auc_roc:.4f}\")\n",
        "\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "print(\"\\nConfusion Matrix (Threshold 0.5):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No Default', 'Default']))\n",
        "\n",
        "print(\"\\nJustification: Ensemble learning (Boosting) provides a stable, highly predictive model (high AUC) which is critical for minimizing costly False Negatives (low Recall on 'Default' class) in financial risk assessment.\")\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch09Tp_MZDrR",
        "outputId": "4e03be6a-95a4-4e36-b3ae-70ccd2ef8004"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating simulated, imbalanced loan default data...\n",
            "Minority class proportion (Default): 2.00%\n",
            "Calculated scale_pos_weight: 49.00\n",
            "\n",
            "Starting GridSearchCV with Stratified K-Fold for Hyperparameter Tuning...\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [12:32:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Tuning and Evaluation Results (XGBoost)\n",
            "==================================================\n",
            "Best Parameters Found: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Test Set AUC-ROC Score: 0.9929\n",
            "\n",
            "Confusion Matrix (Threshold 0.5):\n",
            "[[1952    8]\n",
            " [   4   36]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  No Default       1.00      1.00      1.00      1960\n",
            "     Default       0.82      0.90      0.86        40\n",
            "\n",
            "    accuracy                           0.99      2000\n",
            "   macro avg       0.91      0.95      0.93      2000\n",
            "weighted avg       0.99      0.99      0.99      2000\n",
            "\n",
            "\n",
            "Justification: Ensemble learning (Boosting) provides a stable, highly predictive model (high AUC) which is critical for minimizing costly False Negatives (low Recall on 'Default' class) in financial risk assessment.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WylD0qXSaAV1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}